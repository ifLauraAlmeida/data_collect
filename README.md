<h1 align="center">ğŸ® Data Collect: Resident Evil Database</h1>

Este projeto foi desenvolvido durante o curso **Data Collect** do canal **Teo Me Why**.  
O objetivo Ã© construir um pipeline completo de Engenharia de Dados, desde a coleta via web scraping atÃ© o armazenamento estruturado em mÃºltiplos formatos.

> https://www.youtube.com/watch?v=K-bIZt_hSBo&t=2611s

---
<h1 align="center">âš™ï¸ ConfiguraÃ§Ã£o do Ambiente</h1>

Para garantir reprodutibilidade e isolamento de dependÃªncias, utilizamos **venv**, ferramenta nativa do Python para criaÃ§Ã£o de ambientes virtuais.

- ### 1ï¸âƒ£ CriaÃ§Ã£o do ambiente

```bash
python -m venv .venv
```
- ### 2ï¸âƒ£ AtivaÃ§Ã£o

#### Windows
```bash
.venv\Scripts\activate
```

#### Linux/Mac
```bash
source .venv/bin/activate
```


- #### 3ï¸âƒ£ InstalaÃ§Ã£o das dependÃªncias
```bash
pip install requests beautifulsoup4 pandas tqdm pyarrow fastparquet
```
---
<h1 align="center">ğŸ•·ï¸ Fundamentos da Coleta de Dados</h1>

### ğŸŒ RequisiÃ§Ãµes HTTP e AutenticaÃ§Ã£o

Utilizamos a biblioteca requests para comunicaÃ§Ã£o via protocolo HTTP.
O atributo status_code verifica se a requisiÃ§Ã£o foi bem-sucedida (200). Para contornar bloqueios do site, simulamos uma requisiÃ§Ã£o humana:

`Abrimos o DevTools (F12) â†’ aba Network`

`Copiamos como cURL (POSIX)`

`Convertendo para Python com headers e cookies (ex: User-Agent)`

`Isso permite simular uma sessÃ£o real de navegaÃ§Ã£o.`

<h1 align="center">ğŸ” Parsing com BeautifulSoup</h1>


ApÃ³s obter o HTML via .text, utilizamos BeautifulSoup para navegar na Ã¡rvore DOM:

Localizamos a div principal `(td-page-content)`.

**ExtraÃ­mos:**

- ParÃ¡grafos
- Tags < em > com chaves/valores das caracterÃ­sticas
- Iteramos sobre todos os links da pÃ¡gina principal para automatizar a coleta

<h3 align="center">ğŸ’¾ Armazenamento e Formatos de Arquivo</h3>

Os dados sÃ£o estruturados em um DataFrame do Pandas e exportados para diferentes formatos.

| Formato  | Tipo                 | CaracterÃ­sticas                                                         |
|----------|----------------------|-------------------------------------------------------------------------|
| CSV      | Texto (Plano)        | LegÃ­vel por humanos, nÃ£o preserva tipos de dados e ocupa mais espaÃ§o  |
| Parquet  | BinÃ¡rio (Colunar)    | Compactado, preserva tipos e Ã© otimizado para Big Data                |
| Pickle   | BinÃ¡rio (Serializado)| Salva o estado exato do objeto Python                                 |

âš ï¸ Para Engenharia de Dados, **Parquet Ã© preferÃ­vel** ao CSV devido Ã  **performance e preservaÃ§Ã£o de metadados**, funcionarÃ¡ melhor como um "checkpoint".

---
<h1 align="center">ğŸ§© Estrutura do CÃ³digo</h1>

O script principal Ã© modularizado em funÃ§Ãµes:

- **get_content(url)** â†’ `Realiza requisiÃ§Ã£o HTTP com headers/cookies`

- **get_basic_infos(soup)** â†’ `Extrai descriÃ§Ãµes textuais`

- **get_aparicoes(soup)** â†’ `Mapeia jogos/mÃ­dias onde o personagem aparece`

- **get_links()** â†’ `Coleta URLs de todos os personagens`

---

<h1 align="center">ğŸ“ ConclusÃ£o</h1>

Este projeto foi uma imersÃ£o prÃ¡tica no ciclo de vida inicial do dado:

**ExtraÃ§Ã£o bruta â†’ TransformaÃ§Ã£o â†’ EstruturaÃ§Ã£o â†’ PersistÃªncia**

Durante o desenvolvimento, foram resolvidos desafios reais como bloqueio de requisiÃ§Ãµes automatizadas, mapeamento de elementos no DOM, estruturaÃ§Ã£o leve de dados desorganizados, o resultado Ã© um dataset limpo, estruturado e pronto para anÃ¡lise.

---
<h1 align="center">ğŸš€ Skills Desenvolvidas</h1>

**ğŸ•·ï¸ Web Scraping & AutomaÃ§Ã£o `(requests + BeautifulSoup)`**

**ğŸŒ Engenharia de RequisiÃ§Ãµes `(headers e cookies)`**

**ğŸ“Š Tratamento e estruturaÃ§Ã£o com Pandas**

**ğŸ’¾ SerializaÃ§Ã£o `(CSV vs Parquet vs Pickle)`**

**ğŸ§ª Ambientes isolados com `venv`**

**â³ Monitoramento com `tqdm`**

> Projeto desenvolvido para fins educacionais como prÃ¡tica de Engenharia de Dados.